{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LG Aimers 본선 오프라인 해커톤 코드\n",
    "\n",
    "- 팀명:김승희\n",
    "- 팀원:김승희"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- microsoft/deberta-v3-xsmall Pre-trained 모델을 사용하였습니다. 사전 학습 모델 링크: https://huggingface.co/microsoft/deberta-v3-xsmall  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "004 \n",
    "009 \n",
    "010 \n",
    "012\n",
    "013\n",
    "\n",
    "015 \n",
    "026\n",
    "028 \n",
    "029 \n",
    "032 \n",
    "\n",
    "037 \n",
    "039 \n",
    "040 \n",
    "041 \n",
    "042 \n",
    "\n",
    "043 \n",
    "051 \n",
    "052\n",
    "053 \n",
    "056 \n",
    "\n",
    "064\n",
    "066 \n",
    "068 \n",
    "071 \n",
    "072 \n",
    "073 \n",
    "078 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==2.2.1 \n",
    "!pip install sentence_transformers==2.5.1 \n",
    "!pip install transformers==4.37.1 \n",
    "!pip install tqdm==4.65.0 \n",
    "!pip install pyarrow==15.0.0 \n",
    "!pip install wandb==0.16.3 \n",
    "!pip install spacy==3.7.4 \n",
    "!pip install matplotlib==3.8.3\n",
    "!pip install bitsandbytes==0.41.1 \n",
    "!pip install accelerate==0.21.0 \n",
    "!pip install appdirs==1.4.4 \n",
    "!pip install datasets==2.18.0 \n",
    "!pip install fire==0.5.0 \n",
    "!pip install sentencepiece==0.2.0 \n",
    "!pip install scipy==1.12.0 \n",
    "!pip install numpy==1.24.3 \n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=5 # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_004/'\n",
    "    test_preds_number='test_preds_004.npy'\n",
    "    final_submission_name=\"submission_004.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['new_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max length Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=7 # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_009/'\n",
    "    test_preds_number='test_preds_009.npy'\n",
    "    final_submission_name=\"submission_009.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=9 # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_010/'\n",
    "    test_preds_number='test_preds_010.npy'\n",
    "    final_submission_name=\"submission_010.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=7 # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_012/'\n",
    "    test_preds_number='test_preds_012.npy'\n",
    "    final_submission_name=\"submission_012.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix} is {'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix} is {'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=9 # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_013/'\n",
    "    test_preds_number='test_preds_013.npy'\n",
    "    final_submission_name=\"submission_013.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix} is {'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix} is {'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=7  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_015/'\n",
    "    test_preds_number='test_preds_015.npy'\n",
    "    final_submission_name=\"submission_015.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Data 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_with_is_format(df):\n",
    "    # is_converted가 True인 행들만 선택\n",
    "    df_true = df[df['is_converted'] == True].copy()\n",
    "\n",
    "    # 'new_text' 열에서 ':'을 ' is'로 변경\n",
    "    df_true['new_text'] = df_true['new_text'].str.replace(':', ' is ')\n",
    "\n",
    "    # 원본 데이터프레임에 수정된 행들을 추가\n",
    "    df_augmented = pd.concat([df, df_true], ignore_index=True)\n",
    "    \n",
    "    return df_augmented\n",
    "\n",
    "\n",
    "df_train_text = augment_data_with_is_format(df_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"증강 이후 shape\", df_train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train_text['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=5  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=2\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_026/'\n",
    "    test_preds_number='test_preds_026.npy'\n",
    "    final_submission_name=\"submission_026.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=5  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=4\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_028/'\n",
    "    test_preds_number='test_preds_028.npy'\n",
    "    final_submission_name=\"submission_028.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=7  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=1\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_029/'\n",
    "    test_preds_number='test_preds_029.npy'\n",
    "    final_submission_name=\"submission_029.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=7  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=4\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_032/'\n",
    "    test_preds_number='test_preds_032.npy'\n",
    "    final_submission_name=\"submission_032.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=5  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_037/'\n",
    "    test_preds_number='test_preds_037.npy'\n",
    "    final_submission_name=\"submission_037.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "    \n",
    "    columns = [\n",
    "\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for col_name, prefix in columns:\n",
    "        # 결측치가 아닌 경우에만 해당 열의 데이터를 포함시켜 문자열을 생성\n",
    "        df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{x}.\" if pd.notnull(x) else \"\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "    # for index, (col_name, prefix) in enumerate(columns):\n",
    "    #     if index == 0:  \n",
    "    #         df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "    #     else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "    #         df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    # return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    \n",
    "    if fold==CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=9  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_039/'\n",
    "    test_preds_number='test_preds_039.npy'\n",
    "    final_submission_name=\"submission_039.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "    \n",
    "    columns = [\n",
    "\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for col_name, prefix in columns:\n",
    "        # 결측치가 아닌 경우에만 해당 열의 데이터를 포함시켜 문자열을 생성\n",
    "        df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{x}.\" if pd.notnull(x) else \"\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    \n",
    "    if fold==CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=11  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_040/'\n",
    "    test_preds_number='test_preds_040.npy'\n",
    "    final_submission_name=\"submission_040.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "    \n",
    "    columns = [\n",
    "\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for col_name, prefix in columns:\n",
    "        # 결측치가 아닌 경우에만 해당 열의 데이터를 포함시켜 문자열을 생성\n",
    "        df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{x}.\" if pd.notnull(x) else \"\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    \n",
    "    if fold==CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=7  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_041/'\n",
    "    test_preds_number='test_preds_041.npy'\n",
    "    final_submission_name=\"submission_041.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    ### 아래 2개는 안 씀\n",
    "    # df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    # df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"\"\" Bant submit is The ratio of completed values among the four MQL(Marketing Qualified Lead) components: [1] Budget, [2] Title (customer's position/rank), [3] Needs, and [4] Timeline (desired delivery date).\"\"\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This customer's bant submit is 1.00, indicates that all components (Budget, Title, Needs, Timeline) are fully completed, meaning all required information is provided, fulfilling the MQL criteria perfectly.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant submit is 0.75, indicates that three out of the four components are fully completed. It means that one piece of information is missing or only partially provided.\",\n",
    "        0.50: bant_submit_explain + \" This customer's bant submit is 0.50, indicates that two out of the four components are fully completed. This signifies that only half of the information is fully provided.\",\n",
    "        0.25: bant_submit_explain + \" This customer's bant submit is 0.25, indicates that one out of the four components is fully completed. It suggests that most of the information is missing or not sufficiently provided.\",\n",
    "        0.00: bant_submit_explain + \" This customer's bant submit is 0.00, indicates that none of the components are completed, meaning no information is provided for the MQL criteria.\"\n",
    "    }\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The business unit corresponding to the product requested in the MQL (Marketing Qualified Lead) is {x}.\")\n",
    "\n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" The opportunity ratio calculated based on Vertical Level 1, business unit, and region is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" The id of the customer's company is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The type of customer is {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" Customer's company is a global company.\" if x == \"Enterprise\" else \" Customer's company is a small, or medium-sized company.\")\n",
    "\n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" The number of times previously converted (sales conversion) is {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" 'id_strategic_ver' is 1.0, which means the 'ID' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" 'it_strategic_ver' is 1.0, which means the 'IT' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" This customer has either 'id_strategic_ver' or 'it_strategic_ver' with a value of 1.0.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer's job is {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The total length of the Lead Description text written by the customer is {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" The type of customer inquiry is {x}.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" The requested product category is {x}.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" The requested product subcategory is {x}.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" The model name of the requested product is {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Regional information based on the name of the company's corporation (continent) is {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" The customer's position in the company is {x}.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" The name of the responsible corporation is {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer's requested processing schedule is {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and has a Customer type of an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and belongs to a specific Product Category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" The value multiplied by the ratio of the number of Leads based on Vertical to the overall Lead number and the sales conversion success rate against the number of Leads per Vertical is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" The ratio of the number of sales-converted samples to the number of samples per Business Unit in a specific Vertical Level 1 is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" The customer's business area is {x}.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer's detailed business area is {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" The ID of the salesperson in charge is {x}.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=77\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_042/'\n",
    "    test_preds_number='test_preds_042.npy'\n",
    "    final_submission_name=\"submission_042.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    ### 아래 2개는 안 씀\n",
    "    # df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    # df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"\"\" Bant submit is The ratio of completed values among the four MQL(Marketing Qualified Lead) components: [1] Budget, [2] Title (customer's position/rank), [3] Needs, and [4] Timeline (desired delivery date).\"\"\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This customer's bant submit is 1.00, indicates that all components (Budget, Title, Needs, Timeline) are fully completed, meaning all required information is provided, fulfilling the MQL criteria perfectly.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant submit is 0.75, indicates that three out of the four components are fully completed. It means that one piece of information is missing or only partially provided.\",\n",
    "        0.50: bant_submit_explain + \" This customer's bant submit is 0.50, indicates that two out of the four components are fully completed. This signifies that only half of the information is fully provided.\",\n",
    "        0.25: bant_submit_explain + \" This customer's bant submit is 0.25, indicates that one out of the four components is fully completed. It suggests that most of the information is missing or not sufficiently provided.\",\n",
    "        0.00: bant_submit_explain + \" This customer's bant submit is 0.00, indicates that none of the components are completed, meaning no information is provided for the MQL criteria.\"\n",
    "    }\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The business unit corresponding to the product requested in the MQL (Marketing Qualified Lead) is {x}.\")\n",
    "\n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" The opportunity ratio calculated based on Vertical Level 1, business unit, and region is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" The id of the customer's company is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The type of customer is {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" Customer's company is a global company.\" if x == \"Enterprise\" else \" Customer's company is a small, or medium-sized company.\")\n",
    "\n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" The number of times previously converted (sales conversion) is {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" 'id_strategic_ver' is 1.0, which means the 'ID' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" 'it_strategic_ver' is 1.0, which means the 'IT' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" This customer has either 'id_strategic_ver' or 'it_strategic_ver' with a value of 1.0.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer's job is {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The total length of the Lead Description text written by the customer is {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" The type of customer inquiry is {x}.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" The requested product category is {x}.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" The requested product subcategory is {x}.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" The model name of the requested product is {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Regional information based on the name of the company's corporation (continent) is {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" The customer's position in the company is {x}.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" The name of the responsible corporation is {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer's requested processing schedule is {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and has a Customer type of an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and belongs to a specific Product Category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" The value multiplied by the ratio of the number of Leads based on Vertical to the overall Lead number and the sales conversion success rate against the number of Leads per Vertical is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" The ratio of the number of sales-converted samples to the number of samples per Business Unit in a specific Vertical Level 1 is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" The customer's business area is {x}.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer's detailed business area is {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" The ID of the salesperson in charge is {x}.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=43\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_043/'\n",
    "    test_preds_number='test_preds_043.npy'\n",
    "    final_submission_name=\"submission_043.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" prefer_ver_count is {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" ver_win_rate_mean_upper is {x}.\" if pd.notnull(x) else \"\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"\"\" Bant submit is The ratio of completed values among the four MQL(Marketing Qualified Lead) components: [1] Budget, [2] Title (customer's position/rank), [3] Needs, and [4] Timeline (desired delivery date).\"\"\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This customer's bant submit is 1.00, indicates that all components (Budget, Title, Needs, Timeline) are fully completed, meaning all required information is provided, fulfilling the MQL criteria perfectly.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant submit is 0.75, indicates that three out of the four components are fully completed. It means that one piece of information is missing or only partially provided.\",\n",
    "        0.50: bant_submit_explain + \" This customer's bant submit is 0.50, indicates that two out of the four components are fully completed. This signifies that only half of the information is fully provided.\",\n",
    "        0.25: bant_submit_explain + \" This customer's bant submit is 0.25, indicates that one out of the four components is fully completed. It suggests that most of the information is missing or not sufficiently provided.\",\n",
    "        0.00: bant_submit_explain + \" This customer's bant submit is 0.00, indicates that none of the components are completed, meaning no information is provided for the MQL criteria.\"\n",
    "    }\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The business unit corresponding to the product requested in the MQL (Marketing Qualified Lead) is {x}.\")\n",
    "\n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" The opportunity ratio calculated based on Vertical Level 1, business unit, and region is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" The id of the customer's company is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The type of customer is {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" Customer's company is a global company.\" if x == \"Enterprise\" else \" Customer's company is a small, or medium-sized company.\")\n",
    "\n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" The number of times previously converted (sales conversion) is {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" 'id_strategic_ver' is 1.0, which means the 'ID' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" 'it_strategic_ver' is 1.0, which means the 'IT' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" This customer has either 'id_strategic_ver' or 'it_strategic_ver' with a value of 1.0.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer's job is {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The total length of the Lead Description text written by the customer is {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" The type of customer inquiry is {x}.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" The requested product category is {x}.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" The requested product subcategory is {x}.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" The model name of the requested product is {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Regional information based on the name of the company's corporation (continent) is {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" The customer's position in the company is {x}.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" The name of the responsible corporation is {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer's requested processing schedule is {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and has a Customer type of an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and belongs to a specific Product Category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" The value multiplied by the ratio of the number of Leads based on Vertical to the overall Lead number and the sales conversion success rate against the number of Leads per Vertical is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" The ratio of the number of sales-converted samples to the number of samples per Business Unit in a specific Vertical Level 1 is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" The customer's business area is {x}.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer's detailed business area is {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" The ID of the salesperson in charge is {x}.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=51\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_051/'\n",
    "    test_preds_number='test_preds_051.npy'\n",
    "    final_submission_name=\"submission_051.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description\n",
    "    # df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) and len(x) > 5 else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" prefer_ver_count is {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" ver_win_rate_mean_upper is {x}.\" if pd.notnull(x) else \"\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"\"\" Bant submit is The ratio of completed values among the four MQL(Marketing Qualified Lead) components: [1] Budget, [2] Title (customer's position/rank), [3] Needs, and [4] Timeline (desired delivery date).\"\"\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This customer's bant submit is 1.00, indicates that all components (Budget, Title, Needs, Timeline) are fully completed, meaning all required information is provided, fulfilling the MQL criteria perfectly.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant submit is 0.75, indicates that three out of the four components are fully completed. It means that one piece of information is missing or only partially provided.\",\n",
    "        0.50: bant_submit_explain + \" This customer's bant submit is 0.50, indicates that two out of the four components are fully completed. This signifies that only half of the information is fully provided.\",\n",
    "        0.25: bant_submit_explain + \" This customer's bant submit is 0.25, indicates that one out of the four components is fully completed. It suggests that most of the information is missing or not sufficiently provided.\",\n",
    "        0.00: bant_submit_explain + \" This customer's bant submit is 0.00, indicates that none of the components are completed, meaning no information is provided for the MQL criteria.\"\n",
    "    }\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The business unit corresponding to the product requested in the MQL (Marketing Qualified Lead) is {x}.\")\n",
    "\n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" The opportunity ratio calculated based on Vertical Level 1, business unit, and region is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" The id of the customer's company is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The type of customer is {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" Customer's company is a global company.\" if x == \"Enterprise\" else \" Customer's company is a small, or medium-sized company.\")\n",
    "\n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" The number of times previously converted (sales conversion) is {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" 'id_strategic_ver' is 1.0, which means the 'ID' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" 'it_strategic_ver' is 1.0, which means the 'IT' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" This customer has either 'id_strategic_ver' or 'it_strategic_ver' with a value of 1.0.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer's job is {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The total length of the Lead Description text written by the customer is {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" The type of customer inquiry is {x}.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" The requested product category is {x}.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" The requested product subcategory is {x}.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" The model name of the requested product is {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Regional information based on the name of the company's corporation (continent) is {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" The customer's position in the company is {x}.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" The name of the responsible corporation is {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer's requested processing schedule is {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and has a Customer type of an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and belongs to a specific Product Category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" The value multiplied by the ratio of the number of Leads based on Vertical to the overall Lead number and the sales conversion success rate against the number of Leads per Vertical is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" The ratio of the number of sales-converted samples to the number of samples per Business Unit in a specific Vertical Level 1 is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" The customer's business area is {x}.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer's detailed business area is {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" The ID of the salesperson in charge is {x}.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=52\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_052/'\n",
    "    test_preds_number='test_preds_052.npy'\n",
    "    final_submission_name=\"submission_052.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description -> len()이 5 이하면 그냥 제거\n",
    "    # df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) and len(x) > 5 else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    ### 아래 2개는 안 씀\n",
    "    # df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    # df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"\"\" Bant submit is The ratio of completed values among the four MQL(Marketing Qualified Lead) components: [1] Budget, [2] Title (customer's position/rank), [3] Needs, and [4] Timeline (desired delivery date).\"\"\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This customer's bant submit is 1.00, indicates that all components (Budget, Title, Needs, Timeline) are fully completed, meaning all required information is provided, fulfilling the MQL criteria perfectly.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant submit is 0.75, indicates that three out of the four components are fully completed. It means that one piece of information is missing or only partially provided.\",\n",
    "        0.50: bant_submit_explain + \" This customer's bant submit is 0.50, indicates that two out of the four components are fully completed. This signifies that only half of the information is fully provided.\",\n",
    "        0.25: bant_submit_explain + \" This customer's bant submit is 0.25, indicates that one out of the four components is fully completed. It suggests that most of the information is missing or not sufficiently provided.\",\n",
    "        0.00: bant_submit_explain + \" This customer's bant submit is 0.00, indicates that none of the components are completed, meaning no information is provided for the MQL criteria.\"\n",
    "    }\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The business unit corresponding to the product requested in the MQL (Marketing Qualified Lead) is {x}.\")\n",
    "\n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" The opportunity ratio calculated based on Vertical Level 1, business unit, and region is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" The id of the customer's company is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The type of customer is {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" Customer's company is a global company.\" if x == \"Enterprise\" else \" Customer's company is a small, or medium-sized company.\")\n",
    "\n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" The number of times previously converted (sales conversion) is {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" 'id_strategic_ver' is 1.0, which means the 'ID' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" 'it_strategic_ver' is 1.0, which means the 'IT' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" This customer has either 'id_strategic_ver' or 'it_strategic_ver' with a value of 1.0.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer's job is {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The total length of the Lead Description text written by the customer is {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" The type of customer inquiry is {x}.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" The requested product category is {x}.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" The requested product subcategory is {x}.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" The model name of the requested product is {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Regional information based on the name of the company's corporation (continent) is {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" The customer's position in the company is {x}.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" The name of the responsible corporation is {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer's requested processing schedule is {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and has a Customer type of an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and belongs to a specific Product Category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" The value multiplied by the ratio of the number of Leads based on Vertical to the overall Lead number and the sales conversion success rate against the number of Leads per Vertical is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" The ratio of the number of sales-converted samples to the number of samples per Business Unit in a specific Vertical Level 1 is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" The customer's business area is {x}.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer's detailed business area is {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" The ID of the salesperson in charge is {x}.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 불균형 weight 넣기\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "y_train = df_train_text['is_converted'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # CustomTrainer 정의\n",
    "        class CustomTrainer(Trainer):\n",
    "            def __init__(self, *args, class_weights=None, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.class_weights = class_weights\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.args.device))\n",
    "                    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        # CustomTrainer 인스턴스 생성 및 사용\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary')},\n",
    "            class_weights=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=53\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_053/'\n",
    "    test_preds_number='test_preds_053.npy'\n",
    "    final_submission_name=\"submission_053.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description -> len()이 5 이하면 그냥 제거\n",
    "    # df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) and len(x) > 5 else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" prefer_ver_count is {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" ver_win_rate_mean_upper is {x}.\" if pd.notnull(x) else \"\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"\"\" Bant submit is The ratio of completed values among the four MQL(Marketing Qualified Lead) components: [1] Budget, [2] Title (customer's position/rank), [3] Needs, and [4] Timeline (desired delivery date).\"\"\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This customer's bant submit is 1.00, indicates that all components (Budget, Title, Needs, Timeline) are fully completed, meaning all required information is provided, fulfilling the MQL criteria perfectly.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant submit is 0.75, indicates that three out of the four components are fully completed. It means that one piece of information is missing or only partially provided.\",\n",
    "        0.50: bant_submit_explain + \" This customer's bant submit is 0.50, indicates that two out of the four components are fully completed. This signifies that only half of the information is fully provided.\",\n",
    "        0.25: bant_submit_explain + \" This customer's bant submit is 0.25, indicates that one out of the four components is fully completed. It suggests that most of the information is missing or not sufficiently provided.\",\n",
    "        0.00: bant_submit_explain + \" This customer's bant submit is 0.00, indicates that none of the components are completed, meaning no information is provided for the MQL criteria.\"\n",
    "    }\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The business unit corresponding to the product requested in the MQL (Marketing Qualified Lead) is {x}.\")\n",
    "\n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" The opportunity ratio calculated based on Vertical Level 1, business unit, and region is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" The id of the customer's company is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The type of customer is {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" Customer's company is a global company.\" if x == \"Enterprise\" else \" Customer's company is a small, or medium-sized company.\")\n",
    "\n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" The number of times previously converted (sales conversion) is {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" 'id_strategic_ver' is 1.0, which means the 'ID' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" 'it_strategic_ver' is 1.0, which means the 'IT' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" This customer has either 'id_strategic_ver' or 'it_strategic_ver' with a value of 1.0.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer's job is {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The total length of the Lead Description text written by the customer is {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" The type of customer inquiry is {x}.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" The requested product category is {x}.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" The requested product subcategory is {x}.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" The model name of the requested product is {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Regional information based on the name of the company's corporation (continent) is {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" The customer's position in the company is {x}.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" The name of the responsible corporation is {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer's requested processing schedule is {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and has a Customer type of an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and belongs to a specific Product Category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" The value multiplied by the ratio of the number of Leads based on Vertical to the overall Lead number and the sales conversion success rate against the number of Leads per Vertical is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" The ratio of the number of sales-converted samples to the number of samples per Business Unit in a specific Vertical Level 1 is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" The customer's business area is {x}.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer's detailed business area is {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" The ID of the salesperson in charge is {x}.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 불균형 weight 넣기\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "y_train = df_train_text['is_converted'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # CustomTrainer 정의\n",
    "        class CustomTrainer(Trainer):\n",
    "            def __init__(self, *args, class_weights=None, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.class_weights = class_weights\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.args.device))\n",
    "                    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        # CustomTrainer 인스턴스 생성 및 사용\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary')},\n",
    "            class_weights=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=5  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=56\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_056/'\n",
    "    test_preds_number='test_preds_056.npy'\n",
    "    final_submission_name=\"submission_056.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description -> len()이 5 이하면 그냥 제거\n",
    "    # df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) and len(x) > 5 else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    ### 아래 2개는 안 씀\n",
    "    # df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    # df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"\"\" Bant submit is The ratio of completed values among the four MQL(Marketing Qualified Lead) components: [1] Budget, [2] Title (customer's position/rank), [3] Needs, and [4] Timeline (desired delivery date).\"\"\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This customer's bant submit is 1.00, indicates that all components (Budget, Title, Needs, Timeline) are fully completed, meaning all required information is provided, fulfilling the MQL criteria perfectly.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant submit is 0.75, indicates that three out of the four components are fully completed. It means that one piece of information is missing or only partially provided.\",\n",
    "        0.50: bant_submit_explain + \" This customer's bant submit is 0.50, indicates that two out of the four components are fully completed. This signifies that only half of the information is fully provided.\",\n",
    "        0.25: bant_submit_explain + \" This customer's bant submit is 0.25, indicates that one out of the four components is fully completed. It suggests that most of the information is missing or not sufficiently provided.\",\n",
    "        0.00: bant_submit_explain + \" This customer's bant submit is 0.00, indicates that none of the components are completed, meaning no information is provided for the MQL criteria.\"\n",
    "    }\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The business unit corresponding to the product requested in the MQL (Marketing Qualified Lead) is {x}.\")\n",
    "\n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" The opportunity ratio calculated based on Vertical Level 1, business unit, and region is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" The id of the customer's company is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The type of customer is {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" Customer's company is a global company.\" if x == \"Enterprise\" else \" Customer's company is a small, or medium-sized company.\")\n",
    "\n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" The number of times previously converted (sales conversion) is {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" 'id_strategic_ver' is 1.0, which means the 'ID' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" 'it_strategic_ver' is 1.0, which means the 'IT' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" This customer has either 'id_strategic_ver' or 'it_strategic_ver' with a value of 1.0.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer's job is {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The total length of the Lead Description text written by the customer is {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" The type of customer inquiry is {x}.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" The requested product category is {x}.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" The requested product subcategory is {x}.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" The model name of the requested product is {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Regional information based on the name of the company's corporation (continent) is {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" The customer's position in the company is {x}.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" The name of the responsible corporation is {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer's requested processing schedule is {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and has a Customer type of an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and belongs to a specific Product Category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" The value multiplied by the ratio of the number of Leads based on Vertical to the overall Lead number and the sales conversion success rate against the number of Leads per Vertical is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" The ratio of the number of sales-converted samples to the number of samples per Business Unit in a specific Vertical Level 1 is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" The customer's business area is {x}.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer's detailed business area is {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" The ID of the salesperson in charge is {x}.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 불균형 weight 넣기\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "y_train = df_train_text['is_converted'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # CustomTrainer 정의\n",
    "        class CustomTrainer(Trainer):\n",
    "            def __init__(self, *args, class_weights=None, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.class_weights = class_weights\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.args.device))\n",
    "                    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        # CustomTrainer 인스턴스 생성 및 사용\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary')},\n",
    "            class_weights=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=64\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_064/'\n",
    "    test_preds_number='test_preds_064.npy'\n",
    "    final_submission_name=\"submission_064.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "    \n",
    "    columns = [\n",
    "\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for col_name, prefix in columns:\n",
    "        # 결측치가 아닌 경우에만 해당 열의 데이터를 포함시켜 문자열을 생성\n",
    "        df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{x}.\" if pd.notnull(x) else \"\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 불균형 weight 넣기\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "y_train = df_train_text['is_converted'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # CustomTrainer 정의\n",
    "        class CustomTrainer(Trainer):\n",
    "            def __init__(self, *args, class_weights=None, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.class_weights = class_weights\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.args.device))\n",
    "                    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        # CustomTrainer 인스턴스 생성 및 사용\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary')},\n",
    "            class_weights=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    \n",
    "    if fold==CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=66\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_066/'\n",
    "    test_preds_number='test_preds_066.npy'\n",
    "    final_submission_name=\"submission_066.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "        else:  # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix}:{'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Data 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_with_is_format(df):\n",
    "    # is_converted가 True인 행들만 선택\n",
    "    df_true = df[df['is_converted'] == True].copy()\n",
    "\n",
    "    # 'new_text' 열에서 ':'을 ' is'로 변경\n",
    "    df_true['new_text'] = df_true['new_text'].str.replace(':', ' is ')\n",
    "\n",
    "    # 원본 데이터프레임에 수정된 행들을 추가\n",
    "    df_augmented = pd.concat([df, df_true], ignore_index=True)\n",
    "    \n",
    "    return df_augmented\n",
    "\n",
    "\n",
    "df_train_text = augment_data_with_is_format(df_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"증강 이후 shape\", df_train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train_text['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # trainer 정의\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='binary')},\n",
    "        )\n",
    "        \n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=68\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_068/'\n",
    "    test_preds_number='test_preds_068.npy'\n",
    "    final_submission_name=\"submission_068.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description -> len()이 5 이하면 그냥 제거\n",
    "    # df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) and len(x) > 5 else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" prefer_ver_count is {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" ver_win_rate_mean_upper is {x}.\" if pd.notnull(x) else \"\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"\"\" Bant submit is The ratio of completed values among the four MQL(Marketing Qualified Lead) components: [1] Budget, [2] Title (customer's position/rank), [3] Needs, and [4] Timeline (desired delivery date).\"\"\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This customer's bant submit is 1.00, indicates that all components (Budget, Title, Needs, Timeline) are fully completed, meaning all required information is provided, fulfilling the MQL criteria perfectly.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant submit is 0.75, indicates that three out of the four components are fully completed. It means that one piece of information is missing or only partially provided.\",\n",
    "        0.50: bant_submit_explain + \" This customer's bant submit is 0.50, indicates that two out of the four components are fully completed. This signifies that only half of the information is fully provided.\",\n",
    "        0.25: bant_submit_explain + \" This customer's bant submit is 0.25, indicates that one out of the four components is fully completed. It suggests that most of the information is missing or not sufficiently provided.\",\n",
    "        0.00: bant_submit_explain + \" This customer's bant submit is 0.00, indicates that none of the components are completed, meaning no information is provided for the MQL criteria.\"\n",
    "    }\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The business unit corresponding to the product requested in the MQL (Marketing Qualified Lead) is {x}.\")\n",
    "\n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" The opportunity ratio calculated based on Vertical Level 1, business unit, and region is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" The id of the customer's company is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The type of customer is {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" Customer's company is a global company.\" if x == \"Enterprise\" else \" Customer's company is a small, or medium-sized company.\")\n",
    "\n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" The number of times previously converted (sales conversion) is {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" 'id_strategic_ver' is 1.0, which means the 'ID' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" 'it_strategic_ver' is 1.0, which means the 'IT' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" This customer has either 'id_strategic_ver' or 'it_strategic_ver' with a value of 1.0.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer's job is {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The total length of the Lead Description text written by the customer is {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" The type of customer inquiry is {x}.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" The requested product category is {x}.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" The requested product subcategory is {x}.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" The model name of the requested product is {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Regional information based on the name of the company's corporation (continent) is {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" The customer's position in the company is {x}.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" The name of the responsible corporation is {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer's requested processing schedule is {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and has a Customer type of an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and belongs to a specific Product Category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" The value multiplied by the ratio of the number of Leads based on Vertical to the overall Lead number and the sales conversion success rate against the number of Leads per Vertical is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" The ratio of the number of sales-converted samples to the number of samples per Business Unit in a specific Vertical Level 1 is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" The customer's business area is {x}.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer's detailed business area is {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" The ID of the salesperson in charge is {x}.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 불균형 weight 넣기\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "y_train = df_train_text['is_converted'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # CustomTrainer 정의\n",
    "        class CustomTrainer(Trainer):\n",
    "            def __init__(self, *args, class_weights=None, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.class_weights = class_weights\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.args.device))\n",
    "                    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        # CustomTrainer 인스턴스 생성 및 사용\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary')},\n",
    "            class_weights=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=71\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_071/'\n",
    "    test_preds_number='test_preds_071.npy'\n",
    "    final_submission_name=\"submission_071.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for index, (col_name, prefix) in enumerate(columns):\n",
    "        if index == 0:  \n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\"{prefix} is {'None' if pd.isnull(x) else x}.\")\n",
    "        else:   # 첫 번째 요소가 아닐 때는 앞에 띄어쓰기 추가\n",
    "            df['new_text'] += df[col_name].apply(lambda x: f\" {prefix} is {'None' if pd.isnull(x) else x}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 불균형 weight 넣기\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "y_train = df_train_text['is_converted'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # CustomTrainer 정의\n",
    "        class CustomTrainer(Trainer):\n",
    "            def __init__(self, *args, class_weights=None, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.class_weights = class_weights\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.args.device))\n",
    "                    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        # CustomTrainer 인스턴스 생성 및 사용\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary')},\n",
    "            class_weights=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=72\n",
    "    FOLD=0\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_072/'\n",
    "    test_preds_number='test_preds_072.npy'\n",
    "    final_submission_name=\"submission_072.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수 -> 간단 Version\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    df['new_text'] = \"\"\n",
    "\n",
    "    # 각 컬럼에 대하여 'new_text'에 정보를 추가\n",
    "    # 결측치인 경우 \"None\"을 문자열로 추가\n",
    "    \n",
    "    # 추가된 컬럼\n",
    "# Index([\n",
    "    # \n",
    "    # 'expected_budget', \n",
    "#          'lead_date', \n",
    "#        'customer_history', \n",
    "# #lead_from_channel', \n",
    "# 'lead_description',\n",
    "\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "\n",
    "\n",
    "# 'transfer_agreement',\n",
    "\n",
    "#'ver_win_rate_mean_upper',\n",
    "\n",
    "#       \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = [\n",
    "    # 'expected_budget', -> 돈 어느정도인지. 단위 환산 하면 좋을듯\n",
    "#          'lead_date', -> 날짜\n",
    "#        'customer_history', -> new인지 existing인지 text\n",
    "# #lead_from_channel', -> text\n",
    "# 'lead_description', ->\n",
    "# 'event_name', \n",
    "#'prefer_ver_count',\n",
    "# 'transfer_agreement', Y or N\n",
    "#'ver_win_rate_mean_upper',\n",
    "        \n",
    "        ('lead_description','lead description'),\n",
    "        ('expected_budget', \"expected budget\"),\n",
    "        ('lead_date', \"lead date\"),\n",
    "        ('customer_history', \"customer history\"),\n",
    "        ('lead_from_channel', \"lead from channel\"),\n",
    "        ('event_name', 'event name'),\n",
    "        ('prefer_ver_count', \"prefer ver count\"),\n",
    "        ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        ('bant_submit', \"bant submit\"),\n",
    "        ('customer_country', \"customer country\"),\n",
    "        ('business_unit', \"business unit\"),\n",
    "        ('com_reg_ver_win_rate', \"com_reg_ver_win_rate\"),\n",
    "        ('customer_idx', \"customer idx\"),\n",
    "        ('customer_type', \"customer type\"),\n",
    "        ('enterprise', \"enterprise\"),\n",
    "        ('historical_existing_cnt', \"historical_existing_cnt\"),\n",
    "        ('id_strategic_ver', \"id_strategic_ver\"),\n",
    "        ('it_strategic_ver', \"it_strategic_ver\"),\n",
    "        ('idit_strategic_ver', \"idit_strategic_ver\"),\n",
    "        ('customer_job', \"customer job\"),\n",
    "        ('lead_desc_length', \"lead_desc_length\"),\n",
    "        ('inquiry_type', \"inquiry type\"),\n",
    "        ('product_category', \"product category\"),\n",
    "        ('product_subcategory', \"product subcategory\"),\n",
    "        ('product_modelname', \"product model name\"),\n",
    "        ('customer_country.1', \"customer country.1\"),\n",
    "        ('customer_position', \"customer position\"),\n",
    "        ('response_corporate', \"response corporate\"),\n",
    "        ('expected_timeline', \"expected timeline\"),\n",
    "        ('ver_cus', \"ver_cus\"),\n",
    "        ('ver_pro', \"ver_pro\"),\n",
    "        ('ver_win_rate_x', \"ver_win_rate_x\"),\n",
    "        ('ver_win_ratio_per_bu', \"ver_win_ratio_per_bu\"),\n",
    "        ('business_area', \"business area\"),\n",
    "        ('business_subarea', \"business_subarea\"),\n",
    "        ('lead_owner', \"lead_owner\"),\n",
    "        \n",
    "\n",
    "    ]\n",
    "\n",
    "    for col_name, prefix in columns:\n",
    "        # 결측치가 아닌 경우에만 해당 열의 데이터를 포함시켜 문자열을 생성\n",
    "        df['new_text'] += df[col_name].apply(lambda x: f\"{prefix}:{x}.\" if pd.notnull(x) else \"\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text.to_csv('./text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 불균형 weight 넣기\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "y_train = df_train_text['is_converted'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # CustomTrainer 정의\n",
    "        class CustomTrainer(Trainer):\n",
    "            def __init__(self, *args, class_weights=None, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.class_weights = class_weights\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.args.device))\n",
    "                    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        # CustomTrainer 인스턴스 생성 및 사용\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary')},\n",
    "            class_weights=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"Inferencing fold {fold}\")\n",
    "    model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "    trainer = Trainer(model=model)\n",
    "\n",
    "    # Test dataset에 대한 예측 수행\n",
    "    raw_pred, _, _ = trainer.predict(test_ds)\n",
    "    softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "    \n",
    "    \n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "    else:\n",
    "        test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "    # Fold 하나만 학습하는 경우에는 break\n",
    "    if fold==0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=73\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_073/'\n",
    "    test_preds_number='test_preds_073.npy'\n",
    "    final_submission_name=\"submission_073.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description -> len()이 5 이하면 그냥 제거\n",
    "    # df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) and len(x) > 5 else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    ### 아래 2개는 안 씀\n",
    "    # df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    # df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"\"\" Bant submit is The ratio of completed values among the four MQL(Marketing Qualified Lead) components: [1] Budget, [2] Title (customer's position/rank), [3] Needs, and [4] Timeline (desired delivery date).\"\"\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This customer's bant submit is 1.00, indicates that all components (Budget, Title, Needs, Timeline) are fully completed, meaning all required information is provided, fulfilling the MQL criteria perfectly.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant submit is 0.75, indicates that three out of the four components are fully completed. It means that one piece of information is missing or only partially provided.\",\n",
    "        0.50: bant_submit_explain + \" This customer's bant submit is 0.50, indicates that two out of the four components are fully completed. This signifies that only half of the information is fully provided.\",\n",
    "        0.25: bant_submit_explain + \" This customer's bant submit is 0.25, indicates that one out of the four components is fully completed. It suggests that most of the information is missing or not sufficiently provided.\",\n",
    "        0.00: bant_submit_explain + \" This customer's bant submit is 0.00, indicates that none of the components are completed, meaning no information is provided for the MQL criteria.\"\n",
    "    }\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The business unit corresponding to the product requested in the MQL (Marketing Qualified Lead) is {x}.\")\n",
    "\n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" The opportunity ratio calculated based on Vertical Level 1, business unit, and region is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" The id of the customer's company is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The type of customer is {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" Customer's company is a global company.\" if x == \"Enterprise\" else \" Customer's company is a small, or medium-sized company.\")\n",
    "\n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" The number of times previously converted (sales conversion) is {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" 'id_strategic_ver' is 1.0, which means the 'ID' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" 'it_strategic_ver' is 1.0, which means the 'IT' Business Unit considers the specific Vertical Level 1 area strategically important.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" This customer has either 'id_strategic_ver' or 'it_strategic_ver' with a value of 1.0.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer's job is {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The total length of the Lead Description text written by the customer is {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" The type of customer inquiry is {x}.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" The requested product category is {x}.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" The requested product subcategory is {x}.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" The model name of the requested product is {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Regional information based on the name of the company's corporation (continent) is {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" The customer's position in the company is {x}.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" The name of the responsible corporation is {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer's requested processing schedule is {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and has a Customer type of an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" This customer is within a specific Vertical Level 1 (business area) and belongs to a specific Product Category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" The value multiplied by the ratio of the number of Leads based on Vertical to the overall Lead number and the sales conversion success rate against the number of Leads per Vertical is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" The ratio of the number of sales-converted samples to the number of samples per Business Unit in a specific Vertical Level 1 is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" The customer's business area is {x}.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer's detailed business area is {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" The ID of the salesperson in charge is {x}.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 불균형 weight 넣기\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "y_train = df_train_text['is_converted'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # CustomTrainer 정의\n",
    "        class CustomTrainer(Trainer):\n",
    "            def __init__(self, *args, class_weights=None, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.class_weights = class_weights\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.args.device))\n",
    "                    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        # CustomTrainer 인스턴스 생성 및 사용\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary')},\n",
    "            class_weights=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name='microsoft/deberta-v3-xsmall'\n",
    "    max_length=1024\n",
    "    lr=2e-5\n",
    "    num_labels=2\n",
    "    batch_size=16\n",
    "    epoch=10  # 10\n",
    "    FOLD=0\n",
    "    grad_acc=4\n",
    "    weight_decay=0.01\n",
    "    n_splits=5\n",
    "    seed=78\n",
    "    save_total_limit_num=1\n",
    "    metric_name='f1'\n",
    "    train_only_one_fold=True\n",
    "    lead_desc_first=True\n",
    "    DEMO_TEST=False\n",
    "    \n",
    "    # PATH\n",
    "    train_data_dir='./train.csv'\n",
    "    test_data_dir='./submission.csv'\n",
    "    output_dir='./fold_model_save_078/'\n",
    "    test_preds_number='test_preds_078.npy'\n",
    "    final_submission_name=\"submission_078.csv\"\n",
    "\n",
    "    \n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CFG.train_data_dir)\n",
    "df_test = pd.read_csv(CFG.test_data_dir)\n",
    "\n",
    "### Just for DEMO TEST -> train, test 10개씩\n",
    "if CFG.DEMO_TEST:\n",
    "    df_train = df_train.iloc[:10]\n",
    "    df_test = df_test.iloc[:10]\n",
    "\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 라벨 분포 확인\n",
    "counts = df_train['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data를 new_text Data로 변환하는 함수\n",
    "def tabular_to_new_text_data(df):\n",
    "    \n",
    "    # 초기 'new_text' 컬럼 설정\n",
    "    df['new_text'] = \"Review the diverse information of a customer to determine whether they are a converted customer or not.\"\n",
    "\n",
    "        # ('lead_description','lead description'),\n",
    "        # ('expected_budget', \"expected budget\"),\n",
    "        # ('lead_date', \"lead date\"),\n",
    "        # ('customer_history', \"customer history\"),\n",
    "        # ('lead_from_channel', \"lead from channel\"),\n",
    "        # ('event_name', 'event name'),\n",
    "        # ('prefer_ver_count', \"prefer ver count\"),\n",
    "        # ('transfer_agreement', 'transfer agreement'),\n",
    "        \n",
    "        # ('ver_win_rate_mean_upper', 'ver win rate mean upper'),\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # 고객이 작성한 Lead Description -> len()이 5 이하면 그냥 제거\n",
    "    # df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) else \"\")\n",
    "    df['new_text'] += df['lead_description'].apply(lambda x: f\"Customer's written Lead Description text: {x}.\" if pd.notnull(x) and len(x) > 5 else \"\")\n",
    "\n",
    "    # 고객이 희망하는 예산 범위\n",
    "    df['new_text'] += df['expected_budget'].apply(lambda x: f\" The customer's desired budget range is {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 생성일\n",
    "    df['new_text'] += df['lead_date'].apply(lambda x: f\" Lead information creation date: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # 이전에 영업 전환 되었던 이력 여부\n",
    "    df['new_text'] += df['customer_history'].apply(lambda x: f\" Previous sales conversion history (New / Existing): {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보가 수집된 채널 이름\n",
    "    df['new_text'] += df['lead_from_channel'].apply(lambda x: f\" Lead information was collected from the channel: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # B2B 마케팅 전환 전에 일어난 마케팅 이벤트\n",
    "    df['new_text'] += df['event_name'].apply(lambda x: f\" The marketing event that occurred before the B2B marketing conversion: {x}.\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    # Lead 정보 국외 반출 동의 여부\n",
    "    df['new_text'] += df['transfer_agreement'].apply(lambda x: f\" Consent to the overseas transfer of Lead information: {x}.\" if pd.notnull(x) else \"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    ### 아래 2개는 안 씀\n",
    "    # df['new_text'] += df['prefer_ver_count'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    # df['new_text'] += df['ver_win_rate_mean_upper'].apply(lambda x: f\" Customer's country is {x}.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. bant_submit\n",
    "    bant_submit_explain = \"Bant submission score quantifies the completeness across the four critical MQL factors: [1] Allocation of funds, [2] Designation (customer's level or title), [3] Demands, and [4] Expected delivery timeline.\"\n",
    "\n",
    "    bant_submit_descriptions = {\n",
    "        1.00: bant_submit_explain + \" This client's bant submission level reaches 1.00, showing that all parts (Financial Planning, Job Title, Needs, Project Timeline) have been thoroughly addressed, ensuring that all necessary data is supplied, satisfying the MQL criteria entirely.\",\n",
    "        0.75: bant_submit_explain + \" This customer's bant score of 0.75 signifies that three out of the four critical elements are fully addressed. This indicates an incomplete or partially filled piece of the required information.\",\n",
    "        0.50: bant_submit_explain + \" This client's bant score of 0.50 reveals that half of the critical components, precisely two out of four, are fully filled out, signifying a partial provision of the necessary details.\",\n",
    "        0.25: bant_submit_explain + \" This client's bant score of 0.25 reveals that only one of the four key components is thoroughly completed, implying a substantial absence of necessary information.\",\n",
    "        0.00: bant_submit_explain + \" A bant submit rate of 0.00 for this customer shows that none of the essential elements have been addressed, resulting in a lack of information for meeting the MQL standards.\"\n",
    "    }\n",
    "\n",
    "    df['new_text'] += df['bant_submit'].map(bant_submit_descriptions)\n",
    "\n",
    "    # 2. customer_country\n",
    "    df['new_text'] += df['customer_country'].apply(lambda x: f\" Origin country of the customer is {x}.\")\n",
    "\n",
    "    # 3. business_unit\n",
    "    df['new_text'] += df['business_unit'].apply(lambda x: f\" The {x} division is responsible for the product mentioned in the MQL.\")\n",
    " \n",
    "    # 4. com_reg_ver_win_rate\n",
    "    df['new_text'] += df['com_reg_ver_win_rate'].apply(lambda x: f\" Calculated success ratio across Vertical Level 1, business unit, and geographical area is {x}.\")\n",
    "\n",
    "    # 5. customer_idx\n",
    "    df['new_text'] += df['customer_idx'].apply(lambda x: f\" Customer's company identification number is {x}.\")\n",
    "\n",
    "    # 6. customer_type\n",
    "    df['new_text'] += df['customer_type'].apply(lambda x: f\" The category of the customer falls into {x}.\")\n",
    "\n",
    "    # 7. enterprise\n",
    "    df['new_text'] += df['enterprise'].apply(lambda x: \" This customer operates a worldwide enterprise.\" if x == \"Enterprise\" else \" This customer's business is considered small to medium.\")\n",
    "    \n",
    "    # 8. historical_existing_cnt\n",
    "    df['new_text'] += df['historical_existing_cnt'].apply(lambda x: f\" Historical conversion count stands at {x}.\")\n",
    "\n",
    "    # 9. id_strategic_ver\n",
    "    df['new_text'] += df['id_strategic_ver'].apply(lambda x: \" With 'id_strategic_ver' at 1.0, it's clear that the 'ID' Business Unit values the specific Vertical Level 1 for strategic reasons.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 10. it_strategic_ver\n",
    "    df['new_text'] += df['it_strategic_ver'].apply(lambda x: \" A 'it_strategic_ver' rating of 1.0 underlines the 'IT' Business Unit's strategic focus on the specific Vertical Level 1.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 11. idit_strategic_ver\n",
    "    df['new_text'] += df['idit_strategic_ver'].apply(lambda x: \" For this customer, at least one of 'id_strategic_ver' or 'it_strategic_ver' is marked as 1.0, indicating strategic priority.\" if x == 1.0 else \"\")\n",
    "\n",
    "    # 12. customer_job\n",
    "    df['new_text'] += df['customer_job'].apply(lambda x: f\" This customer holds the position of {x}.\")\n",
    "\n",
    "    # 13. lead_desc_length\n",
    "    df['new_text'] += df['lead_desc_length'].apply(lambda x: f\" The comprehensive length of the Lead Description authored by the customer measures {x}.\")\n",
    "\n",
    "    # 14. inquiry_type\n",
    "    df['new_text'] += df['inquiry_type'].apply(lambda x: f\" Customer's query falls under the {x} category.\")\n",
    "\n",
    "    # 15.product_category\n",
    "    df['new_text'] += df['product_category'].apply(lambda x: f\" Customer has requested a product from the {x} category.\")\n",
    "\n",
    "    # 16.product_subcategory\n",
    "    df['new_text'] += df['product_subcategory'].apply(lambda x: f\" Customer has indicated interest in the {x} subcategory.\")\n",
    "\n",
    "    # 17.product_modelname\n",
    "    df['new_text'] += df['product_modelname'].apply(lambda x: f\" Customer requested the product model {x}.\")\n",
    "\n",
    "    # 18. customer_country.1\n",
    "    df['new_text'] += df['customer_country.1'].apply(lambda x: f\" Continental details, based on the corporation's name, reveal {x}.\")\n",
    "\n",
    "    # 19.customer_position\n",
    "    df['new_text'] += df['customer_position'].apply(lambda x: \"\" if x == \"none\" else f\" This customer holds the title of {x} in the company.\")\n",
    "\n",
    "    # 20.response_corporate\n",
    "    df['new_text'] += df['response_corporate'].apply(lambda x: f\" Responsibility falls under the corporation {x}.\")\n",
    "\n",
    "    # 21. expected_timeline\n",
    "    df['new_text'] += df['expected_timeline'].apply(lambda x: f\" The customer has specified a processing timeframe of {x}.\")\n",
    "\n",
    "    # 22. ver_cus\n",
    "    df['new_text'] += df['ver_cus'].apply(lambda x: \" This client belongs to a particular Vertical Level 1 sector and is identified as an end-user.\" if x == 1 else \"\")\n",
    "\n",
    "    # 23. ver_pro\n",
    "    df['new_text'] += df['ver_pro'].apply(lambda x: \" Situated in a specific Vertical Level 1, this customer is categorized under a distinct product category.\" if x == 1 else \"\")\n",
    "\n",
    "    # 24. ver_win_rate_x\n",
    "    df['new_text'] += df['ver_win_rate_x'].apply(lambda x: f\" A value resulting from the multiplication of the Vertical Leads ratio to total Leads and their conversion success rate is {x}.\")\n",
    "\n",
    "    # 25. ver_win_ratio_per_bu\n",
    "    df['new_text'] += df['ver_win_ratio_per_bu'].apply(lambda x: f\" In a certain Vertical Level 1, the proportion of successfully converted samples against total business unit samples is {x}.\")\n",
    "\n",
    "    # 26. business_area\n",
    "    df['new_text'] += df['business_area'].apply(lambda x: f\" This customer operates within the {x} sector.\")\n",
    "\n",
    "    # 27. business_subarea\n",
    "    df['new_text'] += df['business_subarea'].apply(lambda x: f\" The customer operates in the detailed sector of {x}.\")\n",
    "\n",
    "    # 28. lead_owner\n",
    "    df['new_text'] += df['lead_owner'].apply(lambda x: f\" Assigned salesperson's identification number is {x}.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tabular_to_new_text_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train[['new_text', 'is_converted']]\n",
    "df_train_text['is_converted'] = df_train_text['is_converted'].astype(int)\n",
    "df_train_text.to_csv('./text_and_label.csv')\n",
    "\n",
    "df_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변형한 train dataset에서 가장 긴 문장 가져오기\n",
    "\n",
    "changed_train_text = pd.read_csv('./text_and_label.csv')\n",
    "sample_text_id = changed_train_text['new_text'].apply(len).idxmax()\n",
    "sample_text = changed_train_text.iloc[sample_text_id]['new_text']\n",
    "\n",
    "print('sample_text len() : ', len(sample_text))\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "tokenized_text = tokenizer(sample_text, max_length=1024, padding=True, truncation=True)\n",
    "token_length = len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "print('sample_text token length:', token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 불균형 weight 넣기\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "y_train = df_train_text['is_converted'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 토크나이저, 토큰화 함수\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['new_text'], max_length=CFG.max_length, padding=True, truncation=True)\n",
    "\n",
    "    tokenized_inputs['labels'] = examples['is_converted']\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# StratifiedKFold 설정\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train_text, df_train_text['is_converted'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    \n",
    "    if fold == CFG.FOLD:\n",
    "        # 데이터셋 분할\n",
    "        train_df = df_train_text.iloc[train_index]\n",
    "        valid_df = df_train_text.iloc[valid_index]\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 데이터셋 인코딩\n",
    "        train_ds_enc = train_ds.map(tokenize_function, batched=True)\n",
    "        valid_ds_enc = valid_ds.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # 모델 정의\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name,\n",
    "                                                                num_labels=CFG.num_labels)\n",
    "\n",
    "        \n",
    "        # TrainingArguments 정의\n",
    "        # num_steps = len(train_df) // (CFG.batch_size * CFG.grad_acc)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{CFG.output_dir}fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=CFG.lr,\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            per_device_eval_batch_size=CFG.batch_size,\n",
    "            num_train_epochs=CFG.epoch,\n",
    "            weight_decay=CFG.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=CFG.metric_name,\n",
    "            report_to='none', \n",
    "            save_total_limit = CFG.save_total_limit_num,\n",
    "        )\n",
    "        \n",
    "        # CustomTrainer 정의\n",
    "        class CustomTrainer(Trainer):\n",
    "            def __init__(self, *args, class_weights=None, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.class_weights = class_weights\n",
    "\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.args.device))\n",
    "                    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs.loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        # CustomTrainer 인스턴스 생성 및 사용\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds_enc,\n",
    "            eval_dataset=valid_ds_enc,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: {'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary')},\n",
    "            class_weights=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # 학습과 검증\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        fold_scores.append(eval_result['eval_f1'])\n",
    "        print(f\"Fold {fold} F1 Score: {eval_result['eval_f1']}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(f\"{CFG.output_dir}fold_{fold}_model\")\n",
    "        \n",
    "        # # Fold 하나만 하는 경우 break\n",
    "        # if fold == 0 and CFG.train_only_one_fold:\n",
    "        break\n",
    "\n",
    "# 평균 F1 Score 출력\n",
    "if fold_scores: \n",
    "    print(f\"Fold {CFG.FOLD} F1 Score: {fold_scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 역시 Train과 동일하게 Tabular -> Text 작업을 거친다.\n",
    "df_test = tabular_to_new_text_data(df_test)\n",
    "\n",
    "\n",
    "# ### Text 붙이기\n",
    "# # Customer가 남긴 텍스트 열 이름을 'lead_desc'라고 가정 (오프라인 대회 때 변경)\n",
    "# if CFG.lead_desc_first:\n",
    "#     df_test['new_text'] = \"customer text:\" + df_test['lead_desc'] + \" \" + df_test['new_text']\n",
    "# else:\n",
    "#     df_test['new_text'] = df_test['new_text'] + \" \" + \"customer text:\" + df_test['lead_desc']\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "def prepare_test_data(test_texts):\n",
    "    tokenized_data = tokenizer(test_texts.to_list(), padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "tokenized_test_data = prepare_test_data(df_test['new_text'])\n",
    "test_ds = Dataset.from_dict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(tokenized_test_data[\"input_ids\"]),))\n",
    "\n",
    "# 각 fold 모델에 대한 예측 수행\n",
    "for fold in range(CFG.n_splits):\n",
    "    if fold == CFG.FOLD:\n",
    "        print(f\"Inferencing fold {fold}\")\n",
    "        model_path = f\"{CFG.output_dir}fold_{fold}_model\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=CFG.num_labels)\n",
    "        trainer = Trainer(model=model)\n",
    "\n",
    "        # Test dataset에 대한 예측 수행\n",
    "        raw_pred, _, _ = trainer.predict(test_ds)\n",
    "        softmax_pred = softmax(raw_pred, axis=1)[:, 1]  # 이진 분류인 경우, 클래스 '1'의 확률을 선택\n",
    "        \n",
    "        \n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            test_preds += softmax_pred # Fold 하나만 하는 경우에는 CFG.n_splits 로 나누지 않는다.\n",
    "        else:\n",
    "            test_preds += softmax_pred / CFG.n_splits \n",
    "\n",
    "        # Fold 하나만 학습하는 경우에는 break\n",
    "        if fold==CFG.FOLD and CFG.train_only_one_fold:\n",
    "            break\n",
    "\n",
    "\n",
    "print(test_preds.shape)\n",
    "np.save(CFG.test_preds_number, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 모델 객체 삭제\n",
    "import gc\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_preds_004= np.load('./test_preds_004.npy') \n",
    "test_preds_009= np.load('./test_preds_009.npy') \n",
    "test_preds_010= np.load('./test_preds_010.npy') \n",
    "test_preds_012= np.load('./test_preds_012.npy') \n",
    "test_preds_013= np.load('./test_preds_013.npy') \n",
    " \n",
    "test_preds_015= np.load('./test_preds_015.npy') # 0.025에서 0.547\n",
    "test_preds_026= np.load('./test_preds_026.npy') # 0.05에서 0.538\n",
    "test_preds_028= np.load('./test_preds_028.npy') # 0.02에서 0.532\n",
    "test_preds_029= np.load('./test_preds_029.npy') # 0.02에서 0.532\n",
    "test_preds_032= np.load('./test_preds_032.npy') # 0.04에서 0.530\n",
    "\n",
    "test_preds_037= np.load('./test_preds_037.npy') # 0.01에서 0.544\n",
    "test_preds_039= np.load('./test_preds_039.npy') # 0.05에서 0.532\n",
    "test_preds_040= np.load('./test_preds_040.npy') \n",
    "test_preds_041= np.load('./test_preds_041.npy')\n",
    "test_preds_042= np.load('./test_preds_042.npy') \n",
    "\n",
    "test_preds_043= np.load('./test_preds_043.npy') \n",
    "test_preds_051= np.load('./test_preds_051.npy') \n",
    "test_preds_052= np.load('./test_preds_052.npy') \n",
    "test_preds_053= np.load('./test_preds_053.npy') \n",
    "test_preds_056= np.load('./test_preds_056.npy') \n",
    "\n",
    "test_preds_064= np.load('./test_preds_064.npy') \n",
    "test_preds_066= np.load('./test_preds_066.npy') \n",
    "test_preds_068= np.load('./test_preds_068.npy') \n",
    "test_preds_071= np.load('./test_preds_071.npy') \n",
    "test_preds_072= np.load('./test_preds_072.npy') \n",
    "\n",
    "test_preds_073= np.load('./test_preds_073.npy') \n",
    "test_preds_078= np.load('./test_preds_078.npy') \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_preds = (\n",
    "    \n",
    "    (\n",
    "        \n",
    "\n",
    "\n",
    "    (0.02)*test_preds_004 + \n",
    "    (0.02)*test_preds_009 + \n",
    "    (0.01)*test_preds_010 +\n",
    "    (0.02)*test_preds_012 + \n",
    "    (0.02)*test_preds_013 + \n",
    "    \n",
    "    \n",
    "    (0.06)*test_preds_015  +\n",
    "    (0.03)*test_preds_026  +\n",
    "    (0.03)*test_preds_028 + \n",
    "    (0.02)*test_preds_029 + \n",
    "    (0.03)*test_preds_032 +\n",
    "    \n",
    "    (0.06)*test_preds_037 +\n",
    "    (0.03)*test_preds_039 + \n",
    "    (0.02)*test_preds_040 +\n",
    "    (0.03)*test_preds_041 +\n",
    "    (0.03)*test_preds_042 +\n",
    "    \n",
    "    (0.15)*test_preds_043 +\n",
    "    (0.03)*test_preds_051 +\n",
    "    (0.01)*test_preds_052 +\n",
    "    (0.01)*test_preds_053 +\n",
    "    (0.01)*test_preds_056 +\n",
    "    \n",
    "    (0.02)*test_preds_064 +\n",
    "    (0.16)*test_preds_066 +\n",
    "    (0.01)*test_preds_068 +\n",
    "    (0.02)*test_preds_071 +\n",
    "    (0.07)*test_preds_072 +\n",
    "    \n",
    "    \n",
    "    (0.07)*test_preds_073 +\n",
    "    (0.01)*test_preds_078 \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"./submission.csv\")\n",
    "threshold = 0.163  # 조정할 수 있음\n",
    "df_sub[\"is_converted\"] = (test_preds > threshold).astype(int)\n",
    "\n",
    "counts = df_sub['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"submission.csv\", index=False)\n",
    "counts = df_sub['is_converted'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
